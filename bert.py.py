# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1myv8YjUTD--o0BUHl5Qo1UG3eNlIf474
"""

# Install required libraries
!pip install transformers torch pandas scikit-learn nltk

# Clone the FakeNewsNet repository
!git clone https://github.com/KaiDMML/FakeNewsNet.git

# Import libraries
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW  # Correct import
from transformers import BertTokenizer, BertForSequenceClassification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import os
import nltk
from nltk.corpus import stopwords
import re

# Download NLTK stopwords
nltk.download('punkt')
nltk.download('stopwords')

# Set random seed for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# Define paths to the dataset files
dataset_path = '/content/FakeNewsNet/dataset/'

# Load the datasets
politifact_fake = pd.read_csv(os.path.join(dataset_path, 'politifact_fake.csv'))
politifact_real = pd.read_csv(os.path.join(dataset_path, 'politifact_real.csv'))
gossipcop_fake = pd.read_csv(os.path.join(dataset_path, 'gossipcop_fake.csv'))
gossipcop_real = pd.read_csv(os.path.join(dataset_path, 'gossipcop_real.csv'))

# Add labels: 1 for fake, 0 for real
politifact_fake['label'] = 1
politifact_real['label'] = 0
gossipcop_fake['label'] = 1
gossipcop_real['label'] = 0

# Combine the datasets
data = pd.concat([politifact_fake, politifact_real, gossipcop_fake, gossipcop_real], ignore_index=True)

# Text preprocessing function (minimal, as BERT handles raw text well)
def preprocess_text(text):
    if pd.isna(text):  # Handle missing values
        return ""
    # Convert to lowercase and remove URLs
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    return text.strip()

# Apply preprocessing to the 'title' column
data['cleaned_text'] = data['title'].apply(preprocess_text)

# Remove empty or invalid entries
data = data[data['cleaned_text'] != '']

# Custom Dataset class for BERT
class FakeNewsDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Initialize BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Prepare data
texts = data['cleaned_text'].values
labels = data['label'].values

# Split the data
train_texts, test_texts, train_labels, test_labels = train_test_split(
    texts, labels, test_size=0.2, random_state=42
)

# Create datasets
train_dataset = FakeNewsDataset(train_texts, train_labels, tokenizer)
test_dataset = FakeNewsDataset(test_texts, test_labels, tokenizer)

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16)

# Initialize BERT model for sequence classification
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Set up optimizer
optimizer = AdamW(model.parameters(), lr=2e-5)

# Training loop
def train_model(model, train_loader, epochs=3):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            total_loss += loss.item()

            loss.backward()
            optimizer.step()

        avg_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch + 1}/{epochs}, Average Loss: {avg_loss:.4f}")

# Evaluation function
def evaluate_model(model, test_loader):
    model.eval()
    predictions = []
    true_labels = []
    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            predictions.extend(preds)
            true_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(true_labels, predictions)
    print(f"Accuracy: {accuracy:.4f}")
    print("\nClassification Report:")
    print(classification_report(true_labels, predictions, target_names=['Real', 'Fake']))

# Train the model
print("Training BERT model...")
train_model(model, train_loader)

# Evaluate the model
print("\nEvaluating BERT model...")
evaluate_model(model, test_loader)